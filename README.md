â€‹ğŸ“ Predictive Text Generator using RNN
â€‹This repository features a Deep Learning model built to predict the next word or character in a sequence. Using Recurrent Neural Networks (RNN), the model learns the statistical structure of the input text to generate human-like continuations.
â€‹ğŸš€ Overview
â€‹The core of this project is a sequential model that processes text data by maintaining a "memory" of previous inputs. This allows it to understand context and predict what follows a given "seed" phrase.
â€‹ğŸ§  Model Architecture
â€‹Input Layer: Text sequences converted into numerical tokens.
â€‹RNN/LSTM Layer: The primary engine that captures long-term dependencies in the text.
â€‹Dense Layer: A fully connected layer with a Softmax activation function to predict the probability of the next token.
â€‹Optimization: Trained using Categorical Crossentropy and the Adam optimizer.
â€‹ğŸ› ï¸ Tech Stack
â€‹Language: Python
â€‹Environment: Google Colab
â€‹Libraries: TensorFlow, Keras, NumPy
â€‹ğŸ“Š Results
â€‹As seen in the training logs, the model achieves high accuracy (approx 1.0000 on training batches) and a low loss (approx 0.24), indicating it has successfully memorized and learned the patterns from the provided dataset.
â€‹ğŸ“‚ How to Use
â€‹Clone the repository.
â€‹Open the .ipynb file in Google Colab or Jupyter Notebook.
â€‹Run the cells to train or test the model with your own seed text.
